{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5392ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import shutil\n",
    "from tqdm.notebook  import tqdm\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Lambda, BatchNormalization\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "\n",
    "\n",
    "%load_ext tensorboard\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "895d5371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    r_string = set(doc) - set(['A', 'B', 'C', 'D', 'E', 'F', 'G', \n",
    "                                    'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', \n",
    "                                    'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "                                    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', \n",
    "                                    'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', \n",
    "                                    's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ',\n",
    "                                      '1','2','3','4','5','6','7','8','9', '0','Á', 'Å', 'Æ', 'É',\n",
    "                                  'Í', 'Î', 'Ö', '×', 'Ø', 'Ú', 'Ü', 'Þ', 'à', 'á', 'â', 'ã', 'ä', 'å', \n",
    "                                  'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ø', 'ú',\n",
    "                                  'û', 'ü', 'Ā', 'ā', 'ă', 'ć', 'č', 'Đ', 'đ', 'ė', 'ī', 'Ł', 'ł', 'ń', 'Ō', 'ō',\n",
    "                                  'ś', 'ş', 'š', 'ū', 'ų', 'Ż',\n",
    "                                  'ž', 'ơ', 'ư',\n",
    "                                  'ḥ', 'ṃ', 'ṅ', 'ṣ', 'ṭ', 'ṯ', 'ả', 'ấ', 'ầ', 'ắ', 'ễ', 'ệ', 'ị', 'ớ', 'ử', 'ỳ','½','.',\n",
    "                            \n",
    "                                 ])\n",
    "    r_list = sorted(list(r_string))\n",
    "    print('Removed  --' + ''.join(r_list) +  '\\n')\n",
    "    document_ = doc\n",
    "\n",
    "    for r in r_list:\n",
    "        document_ =  document_.replace(r,\"\")\n",
    "\n",
    "    return document_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58a76e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed  --\n",
      "!\"#$%&'()*+,-/:;<=>?@[\\]^`|~¡£¥§°±²³µ·ʻʿ̃αβγκμСавекостяاحصلنه्กงณตมยรลัาิ่์გდვზიკორსუცძწხჯ჻‑–—‘’“”„†…′″⁄₤€₹⅓⅔→−≤☉♭♯〈〉のァアキスットプュリルヴ・動場大戦攻機殻火礮空隊﻿～\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = tf.io.read_file('./wiki.train.tokens')\n",
    "corpus =tf.strings.lower(corpus)\n",
    "corpus = corpus.numpy()\n",
    "corpus = corpus.decode('utf-8')\n",
    "corpus = clean(corpus)\n",
    "one_doc = [corpus]\n",
    "corpus =  corpus.split('.')\n",
    "corpus = [line for line in corpus if line is not ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93e5e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10347718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'   valkyria chronicles iii    senjō no valkyria 3  unk chronicles  japanese  3  lit . valkyria of th'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(one_doc[0]))\n",
    "one_doc[0][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534db2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   valkyria chronicles iii    senjō no valkyria 3  unk chronicles  japanese  3  lit  \n",
      "\n",
      " valkyria of the battlefield 3   commonly referred to as valkyria chronicles iii outside japan  is a tactical role  playing video game developed by sega and media \n",
      "\n",
      "vision for the playstation portable  \n",
      "\n",
      " released in january 2011 in japan  it is the third game in the valkyria series  \n",
      "\n",
      " unk the same fusion of tactical and real  time gameplay as its predecessors  the story runs parallel to the first game and follows the  nameless   a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit  unk raven   \n",
      "\n",
      "  the game began development in 2010  carrying over a large portion of the work done on valkyria chronicles ii  \n",
      "\n",
      " while it retained the standard features of the series  it also underwent multiple adjustments  such as making the game more unk for series newcomers  \n",
      "\n",
      " character designer unk honjou and composer hitoshi sakimoto both returned from previous entries  along with valkyria chronicles ii director takeshi ozawa  \n",
      "\n",
      " a large team of writers handled the script  \n",
      "\n",
      " the game s opening theme was sung by may n  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(corpus[i] , '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6ffbd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./corpus\" +'_file.pkl'\n",
    "with open(path, 'wb') as files:\n",
    "    pickle.dump(corpus, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ec8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./corpus\" +'_file.pkl'\n",
    "with open(path, 'rb') as files:\n",
    "     corpus = pickle.load(files)\n",
    "        \n",
    "max_document_length =16\n",
    "\n",
    "max_document_length = 10347718\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word2id = tokenizer.word_index\n",
    "word2id['PAD'] = 0\n",
    "\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[ word2id[w] for w in text.text_to_word_sequence(doc)] \n",
    "        for doc in corpus]\n",
    "\n",
    "\n",
    "# wids = sequence.pad_sequences(wids, maxlen=max_document_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cec4919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3768, 3831, 859, 18089, 75, 3768, 80, 3, 3831, 759]\n"
     ]
    }
   ],
   "source": [
    "(len(wids[0]))\n",
    "print(wids[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c102a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 28673\n",
      "Vocabulary Sample: [('the', 1), ('of', 2), ('unk', 3), ('and', 4), ('in', 5), ('to', 6), ('a', 7), ('was', 8), ('on', 9), ('as', 10)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2id)\n",
    "embedding_dim = 512\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "600ee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=2\n",
    "    \n",
    "    # def generate_context_word_pairs(corpus=wids, window_size=3, vocab_size=vocab_size):\n",
    "#     context_length = window_size*2 -1   \n",
    "#     for doc in corpus:\n",
    " \n",
    "#         sentence_length = doc.shape[0]\n",
    "#         # print(\"sentence_length\" , sentence_length)\n",
    "#         context_words = []\n",
    "#         label_words = []\n",
    "#         # print(doc)\n",
    "\n",
    "#         for index, word in enumerate(doc):\n",
    "#             # print(\"index\" , index)\n",
    "            \n",
    "#             start = index - window_size\n",
    "#             end = index + window_size\n",
    "            \n",
    "#             # print(\"start\" , start)\n",
    "#             # print(\"end\" , end)\n",
    "            \n",
    "#             input_ = []\n",
    "#             for i in range(start, end) :\n",
    "#                 if (0 <= i and i< sentence_length ):\n",
    "#                     # print(i)\n",
    "#                     if(i != index):\n",
    "#                         input_.append(doc[i])\n",
    "\n",
    "\n",
    "#             context_words.append(input_)\n",
    "#             label_words.append(word)\n",
    "\n",
    "#         # print(context_words)\n",
    "#         x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "#         y = np_utils.to_categorical(label_words, vocab_size)\n",
    "\n",
    "#         yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f737e230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 1024\n",
    "\n",
    "def generate_context_word_pairs(corpus=wids, window_size=3, vocab_size=vocab_size):\n",
    "    \n",
    "    context_length = window_size*2 -1\n",
    "\n",
    "   \n",
    "    for doc in corpus:\n",
    " \n",
    "        sentence_length = len(doc)\n",
    "        # print(\"sentence_length\" , sentence_length)\n",
    "        context_words = []\n",
    "        label_words = []\n",
    "        # print(doc)\n",
    "        batch_i = 0\n",
    "\n",
    "        for index, word in enumerate(doc):\n",
    "#             print(\"index\" , index)\n",
    "#             print(\"word\" , index)\n",
    "            \n",
    "            start = index - window_size\n",
    "            end = index + window_size\n",
    "            \n",
    "            # print(\"start\" , start)\n",
    "            # print(\"end\" , end)\n",
    "            \n",
    "            input_ = []\n",
    "            for i in range(start, end) :\n",
    "                if (0 <= i and i< sentence_length ):\n",
    "                    # print('word', doc[i])\n",
    "                    \n",
    "                    if(i != index):\n",
    "                        input_.append(doc[i])\n",
    "                        \n",
    "            # print(input_)\n",
    "                        \n",
    "            context_words.append(input_)\n",
    "            label_words.append(word)\n",
    "            \n",
    "            batch_i += 1\n",
    "            if(batch_i == batch_size):\n",
    "                x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "                y = np_utils.to_categorical(label_words, vocab_size)\n",
    "                batch_i = 0\n",
    "                context_words = []\n",
    "                label_words = [] \n",
    "                \n",
    "                yield x, y\n",
    "            \n",
    "        \n",
    "        \n",
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        print(x, y)\n",
    "        print(len(x))\n",
    "        print(len(y[0]))\n",
    "\n",
    "        # print('Context (X):', [id2word[w] for w in x[5]], '-> Target (Y):', y[5])\n",
    "        \n",
    "        if i == 2:\n",
    "            break\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a6b085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "w2v_embedding (Embedding)    (None, 4, 512)            14680576  \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28673)             14709249  \n",
      "=================================================================\n",
      "Total params: 29,391,873\n",
      "Trainable params: 29,390,849\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build CBOW architecture\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length= window_size*2, name = 'w2v_embedding'))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_dim,)))\n",
    "cbow.add(tf.keras.layers.BatchNormalization())\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[acc])\n",
    "\n",
    "# view model summary\n",
    "print(cbow.summary())\n",
    "\n",
    "logs = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(logs + \"/metrics\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3d1fae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "epoch = 0\n",
    "losses= []\n",
    "accuracies = []\n",
    "step = 0\n",
    "win_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "24a21b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir logs\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ced047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "while epoch < epochs:\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    epoch += 1\n",
    "   \n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=win_size, vocab_size=vocab_size):\n",
    "\n",
    "        step = step + 1\n",
    "        with tf.GradientTape() as tape:\n",
    "   \n",
    "            y_ = cbow(x) \n",
    "            loss_value = loss(y, y_)\n",
    "        \n",
    "            grads = tape.gradient(loss_value, cbow.trainable_weights)    \n",
    "            opt.apply_gradients(zip(grads, cbow.trainable_weights))\n",
    "        \n",
    "        acc.update_state(y, y_)\n",
    "        train_acc = acc.result()\n",
    "\n",
    "        losses.append(loss_value)\n",
    "        accuracies.append(train_acc )\n",
    "       \n",
    "        print('Loss {}   Accuracy {}'.format(loss_value ,train_acc*100))\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        with file_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss_value, step =step)\n",
    "            tf.summary.scalar('accuracy', train_acc, step = step)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "00c1c4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\cbow\\assets\n"
     ]
    }
   ],
   "source": [
    "cbow.save('.\\cbow')\n",
    "np.save('Loss',losses)\n",
    "np.save('Accuracy',accuracies)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5c6d510e28c9b9013be6c2358d02dd554c357c981f1197507ba2aa8a3a7adb2"
  },
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
